# Describe Design Considerations For Applications That Use Foundation Models (FMs)
ü§ñ **Exam Guide:** AI Practitioner
**Domain 3: Applications of Foundation Models**
üìò_Task Statement 3.1_

## üéØ Objectives

This task shifts from ‚Äúwhat GenAI is‚Äù (Domain 2) to ‚Äúhow to design real applications with it‚Äù (Domain 3). 

You‚Äôre expected to understand how to pick a model, control its behaviour, ground it with RAG, store embeddings, choose customization strategies, and use agents for multi-step workflows.

---

### 1) Selection Criteria For Choosing Pre-trained Models

When selecting a pre-trained FM/LLM, consider:

#### 1.1 **Cost**
Often driven by token pricing (input/output) and model tier.
_Higher capability typically costs more._

#### 1.2 **Modality**
Text-only vs **multimodal** (text+image) vs image generation, etc.
_Choose based on required input/output types._

#### 1.3 **Latency**
Interactive assistants need low latency while batch workloads can tolerate more delay.

#### 1.4 **Multilingual Support**
If you serve global users, ensure strong performance in required languages.

#### 1.5 **Model Size / Complexity**
Larger models can be more capable but slower and more expensive.
_Smaller models can be cheaper/faster and ‚Äúgood enough‚Äù for narrow tasks._

#### 1.6 **Customization Options**

1. Can you fine-tune? 
2. Is prompt engineering sufficient? 
3. Does the service support your needed controls?

#### 1.7 **Input/output length**
Context window size determines how much the model can ‚Äúsee.‚Äù
_Output limits matter for summarization length, structured responses, etc._

#### 1.8 **Prompt Caching** 
*(if available in your architecture/service)*
Reusing repeated prompt prefixes can reduce cost/latency for common templates (e.g., same system instructions used for every request).

---

### 2) Effect Of Inference Parameters On Model Responses

Inference parameters change the ‚Äústyle‚Äù and consistency of outputs:

#### 2.1 **Temperature**
Controls randomness/creativity.
  - **Lower temperature** ‚Üí more consistent, safer, more deterministic-like outputs (often better for extraction/structured formats).
  - **Higher temperature** ‚Üí more varied/creative outputs (often better for brainstorming, marketing copy).

#### 2.2 **Input Length**
More input context can improve grounding, but increases cost and can introduce noise if irrelevant.
_Longer prompts can also increase latency._

#### 2.3 **Output Length**
Limits how much the model can generate.
_Longer outputs cost more and may increase the chance of drifting off-topic and shorter outputs may omit needed details._

know the directionality:
**temperature up:** _more variability_
**more tokens:** _more cost_

---

### 3) Retrieval Augmented Generation (RAG)

#### 3.1 **Definition**
**RAG** is an approach where the model‚Äôs response is generated using:
**_1_** **retrieved** relevant information from an external knowledge source (documents, wikis, policies, tickets), and  
**_2_** the FM generates an answer grounded in that retrieved context.

#### 3.2 **Why Businesses Use RAG**
**_1_** Keeps answers aligned with **company-specific** and **up-to-date** information.
**_2_** Reduces hallucinations by grounding outputs in retrieved text.
**_3_** Avoids retraining/fine-tuning for every knowledge update.

#### 3.3 **Business applications**
**_1_** Knowledge base Q&A (HR policies, IT runbooks, product docs)
**_2_** Customer support assistants (answers based on help-center articles)
**_3_** Contract/policy summarization with citations to source text
**_4_** Research assistants over internal document repositories

#### 3.4 **AWS Example**
**Amazon Bedrock Knowledge Bases**  provide managed building blocks for RAG-style workflows.

---

### 4) AWS Services For Storing Embeddings In Vector Databases

RAG typically requires storing embeddings so you can do similarity search. AWS services commonly referenced include:

#### 4.1 **Amazon OpenSearch Service**
Often used for search + vector similarity search use cases.

#### 4.2 **Amazon Aurora**
Can be used to store vectors and metadata depending on engine/features.

#### 4.3 **Amazon RDS for PostgreSQL**
PostgreSQL is commonly used for vector storage and similarity search (engine/extensions depend on configuration).

#### 4.4 **Amazon Neptune**
Amazon Neptune is a graph database that can support relationship-centric retrieval and can be part of retrieval strategies involving connected data.

#### 4.5 **Amazon DocumentDB (with MongoDB compatibility)**
Amazon DocumentDB can be used to store content/metadata and support retrieval patterns depending on features/architecture.

_Recognize these as AWS options for vector/embedding storage‚Äînot deep implementation details._

---

### 5) Cost Tradeoffs Of FM Customization Approaches

You‚Äôre expected to understand that there are multiple ways to ‚Äúcustomize‚Äù outcomes, each with different cost/effort:

#### 5.1 **Pre-training**
Training a foundation model from scratch.
_**Highest cost** and complexity, usually only large providers do this._

#### 5.2 **Fine-tuning**
Adjusting a pre-trained model on your domain/task data, which usually improves consistency, domain style, and task performance.
_**Costs:** training + governance + ongoing maintenance._

#### 5.3 **In-Context Learning (Prompting / Few-shot)**
Provide instructions and examples in the prompt.
_**Fastest and lowest engineering overhead**, but increases token usage and may be less consistent at scale._

#### 5.4 **RAG**
Use retrieval to ground responses in external knowledge, which is often cheaper and more maintainable than fine-tuning for knowledge updates.
_**Costs:** embedding generation + vector storage + retrieval + added prompt tokens._

**_1_** If the problem is _‚Äúthe model doesn‚Äôt know our private docs‚Äù_ ‚Üí start with **RAG**.
**_2_** If the problem is _‚Äúthe model won‚Äôt follow our format/tone reliably‚Äù_ ‚Üí consider **fine-tuning** or stricter prompting + validation.
**_3_** If the problem is _‚Äúwe need a brand-new capability at massive scale‚Äù_ ‚Üí that‚Äôs closer to **pre-training** (rare).

---

### 6) Role Of Agents In Multi-Step Tasks

**Agents** extend a model from _‚Äúresponding‚Äù_ to **‚Äúacting.‚Äù**
Agents can plan and execute **multi-step workflows**, calling tools/APIs along the way.
Agents are useful when tasks require:
  **_1_** looking up information,
  **_2_** taking actions (create ticket, book meeting),
  **_3_** iterating through steps,
  **_3_** using multiple systems.

#### **Examples**
**_1_** **Agents for Amazon Bedrock**: managed capability to build agentic workflows around FMs.
**_2_** **Agentic AI**: LLM + tools + planning + memory/context.
**_3_** **Model Context Protocol (MCP)**: a standard pattern for connecting models to external tools/context providers (exam-level awareness only).

---

### üí° **Quick Questions**

**1.** Name three criteria you would use to select a pre-trained foundation model for a customer-facing assistant.
**2.** What does **temperature** control, and what happens when you increase it?
**3.** What is **RAG**, and why do organizations use it instead of retraining models for new documents?
**4.** Name two AWS services that can be used to store embeddings for vector similarity search.
**5.** In one sentence, what is the role of an **agent** in an FM-based application?


## Additional Resources
1. [Prompt caching for faster model inference](https://docs.aws.amazon.com/bedrock/latest/userguide/prompt-caching.html)
2. [What is Prompt Caching?](https://www.ibm.com/think/topics/prompt-caching)
3. [Prompt caching](https://platform.openai.com/docs/guides/prompt-caching)
4. [Prompt Caching: A Guide With Code Implementation](https://www.datacamp.com/tutorial/prompt-caching)
5. [Prompt caching](https://platform.claude.com/docs/en/build-with-claude/prompt-caching)
6. [Amazon Bedrock Knowledge Bases](https://aws.amazon.com/bedrock/knowledge-bases/)
7. [Vector database options](https://docs.aws.amazon.com/prescriptive-guidance/latest/choosing-an-aws-vector-database-for-rag-use-cases/vector-db-options.html)
8. [How Amazon Bedrock knowledge bases work](https://docs.aws.amazon.com/bedrock/latest/userguide/kb-how-it-works.html)
9. [Amazon Bedrock Agents](https://aws.amazon.com/bedrock/agents/)
10. [Amazon Bedrock AgentCore Beginner's Guide - AI Agent Development from Basics with Detailed Term Explanations](https://hidekazu-konishi.com/entry/amazon_bedrock_agentcore_beginners_guide.html)

### ‚úÖ _Answers to Quick Questions_

**1.** **Latency** (fast enough for interactive use), **cost** (token pricing/usage), and **multilingual support** (if serving multiple languages).  
   *(Also valid: modality, input/output length, model size/complexity, customization options, prompt caching.)*

**2.** **Temperature controls randomness/creativity** in the model‚Äôs outputs. Increasing it generally makes responses **more varied and less predictable** (often more creative, but potentially less consistent).

**3.** **RAG (Retrieval Augmented Generation)** retrieves relevant information from an external knowledge source and provides it to the model to generate a grounded response. Organizations use it because it can incorporate **updated/internal documents without retraining**, reducing cost and improving factual grounding.

**4.** **Amazon OpenSearch Service** and **Amazon RDS for PostgreSQL**.  
   *(Also valid: Amazon Aurora, Amazon Neptune, Amazon DocumentDB.)*

**5.** An **agent** enables the model to perform **multi-step tasks** by planning and calling external tools/APIs to take actions, not just generate text.

# The Original

**Blog:** [Ntombizakhona Mabaso](https://dev.to/ntombizakhona)
<br>
**Article Link:** [Describe Design Considerations For Applications That Use Foundation Models (FMs)](https://dev.to/aws-builders/describe-design-considerations-for-applications-that-use-foundation-models-fms-4jna)
<br>
Originally Published by [Ntombizakhona Mabaso](https://dev.to/ntombizakhona)
<br>
**20 January 2026**
