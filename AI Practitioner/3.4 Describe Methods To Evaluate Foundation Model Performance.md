ü§ñ **Exam Guide:** AI Practitioner
**Domain 3: Applications of Foundation Models**
üìò_Task Statement 3.4_

## üéØ Objectives
This task is about evaluating whether an FM (and an FM-powered application like RAG or an agent) is *good enough*, not just in a demo, but against repeatable criteria using human review, benchmarks, and objective metrics tied to business goals.

---

### 1) Approaches To Evaluate FM performance

#### 1.1 **Human Evaluation**
People review model outputs and score them against criteria such as:
  **_1_** correctness/helpfulness
  **_2_** clarity and tone
  **_3_** completeness
  **_4_** safety/policy compliance
**Best for:** subjective qualities and real user experience.
**Tradeoff:** _slower and more expensive, but often the most reliable indicator._

#### 1.2 **Benchmark Datasets**
Use a fixed dataset (prompts + expected outputs) and evaluate consistently over time.
**Best for:** regression testing, comparing models/prompts, measuring improvement.
**Tradeoff:** _benchmarks may not reflect your exact domain and can be ‚Äúgamed‚Äù if over-optimized._

#### 1.3 **Amazon Bedrock Model Evaluation**
A managed way to evaluate and compare model outputs,
**Useful for** standardized evaluation workflows across prompts/models.

---

### 2) Metrics To Assess FM Performance
These metrics compare generated text to a reference (ground truth). They‚Äôre most useful when you *have* expected answers (e.g., translation pairs, reference summaries).

#### 2.1 **ROUGE** 
##### **Recall-Oriented Understudy for Gisting Evaluation**
**ROUGE** is common for **summarization**, it measures overlap between generated and reference text (often n-grams).
**Recall-oriented:** _focuses on how much of the reference content was captured._

#### 2.2 **BLEU (Bilingual Evaluation Understudy)**
**BLEU** is common for **machine translation**, it measures n-gram precision (how much generated text matches the reference translation).

#### 2.3 **BERTScore**
**BERTScore** uses embeddings from transformer models (e.g., BERT-like) to measure **semantic similarity**, not just exact word overlap.
**Useful when** _wording differs but meaning is similar._

_**No single metric proves ‚Äúquality.‚Äù Metrics can be paired with human evaluation and task-specific checks.**_

---

### 3) Determine Whether The FM Meets Business Objectives

FM quality must translate into outcomes that matter.

Examples of business objective alignment:

#### 3.1 **Productivity**
Time saved per task, faster drafting, fewer manual steps, reduced handling time.

#### 3.2 **User Engagement**
Retention, session length, repeat usage, satisfaction ratings.

#### 3.3 **Task Success / Task Engineering**
Whether users can reliably complete the intended task (e.g., extract fields correctly, create a correct ticket, answer questions with citations).

_**A model can score well on ROUGE/BLEU but still fail business goals if it‚Äôs too slow, too expensive, unsafe, or doesn‚Äôt improve user outcomes.**_

---

### 4) Evaluating FM-based Applications 
#### RAG, agents, workflows

It‚Äôs not enough to evaluate the base model, you must evaluate the *system*.

#### 4.1 **RAG Evaluation**
Evaluate:
**_1_** **Retrieval Quality:** Are the right documents/chunks being retrieved?
**_2_** **Grounding:** Does the answer use the retrieved context _and avoid making things up_?
**_3_** **Answer Quality:** Correctness, completeness, citations, and refusal behavior when context is missing.

#### 4.2 **Common Application Metrics:**
**_1_** grounded answer rate / citation accuracy
**_2_** retrieval recall/precision (did we fetch relevant chunks?)
**_3_** hallucination rate (answers not supported by sources)

#### 4.3 **Agent / Workflow Evaluation**
Evaluate:
**_1_** **Task completion rate:** Did the agent finish the multi-step objective?
**_2_** **Tool correctness:** Did it call the right tool with correct parameters?
**_3_** **Safety/compliance:** Did it attempt disallowed actions or expose sensitive data?
**_4_** **Efficiency:** Number of steps/tool calls, latency, cost per completed task.

---

### üí° **Quick Questions**

**1.** What‚Äôs one advantage of **human evaluation** over automated metrics?
**2.** Which metric is commonly associated with **summarization**: ROUGE or BLEU?
**3.** What is **BERTScore** trying to measure that ROUGE/BLEU may miss?
**4.** Name one way to evaluate a **RAG** application beyond ‚Äúis the answer good?‚Äù
**5.** Give one example of a **business metric** you might use to evaluate an FM-powered assistant.



## Additional Resources
1. [Evaluate, compare, and select the best foundation models for your use case in Amazon Bedrock (preview)](https://aws.amazon.com/blogs/aws/evaluate-compare-and-select-the-best-foundation-models-for-your-use-case-in-amazon-bedrock-preview/)
2. [Amazon Bedrock Evaluations](https://aws.amazon.com/bedrock/evaluations/)
3. [Review metrics for an automated model evaluation job in Amazon Bedrock (console)](https://docs.aws.amazon.com/bedrock/latest/userguide/model-evaluation-report-programmatic.html)
4. [Evaluate the text summarization capabilities of LLMs for enhanced decision-making on AWS](https://aws.amazon.com/blogs/machine-learning/evaluate-the-text-summarization-capabilities-of-llms-for-enhanced-decision-making-on-aws/)
5. [Accuracy](https://docs.aws.amazon.com/sagemaker/latest/dg/clarify-accuracy-evaluation.html)
6. [Evaluate models or RAG systems using Amazon Bedrock Evaluations ‚Äì Now generally available](https://aws.amazon.com/blogs/machine-learning/evaluate-models-or-rag-systems-using-amazon-bedrock-evaluations-now-generally-available/)
7. [Evaluate and improve performance of Amazon Bedrock Knowledge Bases](https://aws.amazon.com/blogs/machine-learning/evaluate-and-improve-performance-of-amazon-bedrock-knowledge-bases/)

### ‚úÖ _Answers to Quick Questions_

**1.** Humans can judge qualities that automated metrics struggle with, such as **helpfulness, tone, clarity, policy compliance, and real-world correctness** in context.

**2.** **ROUGE** is commonly used for **summarization**.  
   *(BLEU is more commonly used for translation.)*

**3.** **Semantic similarity/meaning**, even when the wording is different (not just exact n-gram overlap).

**4.** Evaluate **retrieval quality** (e.g., whether the system retrieves the most relevant chunks/documents) and **grounding** (whether answers are supported by retrieved sources).

**5.** **Productivity/time saved** (e.g., reduced average handling time), **user engagement/retention**, or **conversion rate** (depending on the use case).

# The Original

**Blog:** [Ntombizakhona Mabaso](https://dev.to/ntombizakhona)
<br>
**Article Link:** [Describe Methods To Evaluate Foundation Model Performance](https://dev.to/aws-builders/describe-methods-to-evaluate-foundation-model-performance-1chb)
<br>
Originally Published by [Ntombizakhona Mabaso](https://dev.to/ntombizakhona)
<br>
**22 January 2026**
